{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "sentiment_prediction.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VighneshS/sentiment_prediction/blob/master/sentiment_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "YMJhP0QcWtNL"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VighneshS/sentiment_prediction/blob/master/sentiment_prediction.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "<a href=\"https://vighnesh-studies.blogspot.com/2021/04/sentiment-prediction-using-naive-bayes.html\" target=\"_blank\">BLOG</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "eE8QTqCtWtNO"
   },
   "source": [
    "# Sentiment Prediction using Naive Bayes Classifier (NBC)\n",
    "This is a notebook to understand how Naive Bayes Classifier (NBC) works and also how it is useful to classify text based on sentiment.\n",
    "\n",
    "We will also see how it will be effective against missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ubHEGALQWtNP"
   },
   "source": [
    "## Settings\n",
    "Training Percentage"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9dnquo1pWtNP"
   },
   "source": [
    "TRAINING_RATIO = 80 / 100\n",
    "K_FOLDS = 5\n",
    "MOST_USEFUL_LIMIT = 20\n",
    "\n",
    "MOST_COMMON_WORDS_IN_DATA_SET = [\"movie\", \"film\", \"one\"]"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CONSTANTS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "REVIEW_COL = \"IMDB Review\"\n",
    "WORD_FREQ_COL = \"Word Frequency\"\n",
    "SENTIMENT_COL = \"Sentiment\"\n",
    "POS_SENTIMENT_WORD_FREQ_COL = \"Positive Sentiment Word Frequency\"\n",
    "NEG_SENTIMENT_WORD_FREQ_COL = \"Negative Sentiment Word Frequency\"\n",
    "P_SENTIMENT_POSITIVE_COL = \"P(Sentiment = Positive)\"\n",
    "P_SENTIMENT_NEGATIVE_COL = \"P(Sentiment = Negative)\"\n",
    "P_WORD_COL = \"P(Word)\"\n",
    "WORD_COL = \"Word\"\n",
    "P_WORD_GIVEN_SENTIMENT_POSITIVE_COL = \"P(Word | Sentiment = Positive)\"\n",
    "P_WORD_GIVEN_SENTIMENT_NEGATIVE_COL = \"P(Word | Sentiment = Negative)\"\n",
    "P_SENTIMENT_POSITIVE_GIVEN_SENTENCE_COL = \"P(Sentiment = Positive | Sentence)\"\n",
    "P_SENTIMENT_NEGATIVE_GIVEN_SENTENCE_COL = \"P(Sentiment = Negative | Sentence)\"\n",
    "PREDICTED_SENTIMENT_COL = \"Predicted sentiment\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "buzL7wewWtNP"
   },
   "source": [
    "## Importing the Data\n",
    "We used the [kaggle dataset](https://storage.googleapis.com/kagglesdsdata/datasets/22169/30047/sentiment%20labelled%20sentences/imdb_labelled.txt?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210425%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210425T202010Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=6133706ef10bc2dcd0b58f8398b4d73ab9e9d788de1718b07334df91f6007e1e4ca0b78e3176f95b8250e0c4535ce1633528f4fabffeb7e4124af3ee3f895ac34c03044fca9b23b23c4ddb8fa90d84dfc14869ff4806f03783cafad53b19445b3c3052983fdf1ca4384257eac1bc0a4270d238a1ea89d1289866c7a0ea7ad7c97a76f2e142c148019e39cc5a1295f92650747ac5ea5946b026f7ad6d5d262d4c4a370aee6bc1f5d5b445bb6d93692debe678a79e5e1c1fe3d3e68ea4f2fad3115795d3361e0626e98156fbc7f5967beb7cf0f00e07351d23a00d8677ebb75e3e13b1bfa07762266efabf6f6f9d53206be31b7623cf3614f60f8cf5011cf23def) to get the ground truth of sample IMDB reviews."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "TSRAy6m8WtNP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "67167b51-9660-490f-c040-1ac113712c4a"
   },
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from IPython.display import display\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HhyN6hKLXIwn"
   },
   "source": [
    "stop_words = stopwords.words('english')"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - '/Users/jeevavighnesh/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/corpus/util.py\u001B[0m in \u001B[0;36m__load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     82\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 83\u001B[0;31m                     \u001B[0mroot\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"{}/{}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msubdir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mzip_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     84\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/data.py\u001B[0m in \u001B[0;36mfind\u001B[0;34m(resource_name, paths)\u001B[0m\n\u001B[1;32m    582\u001B[0m     \u001B[0mresource_not_found\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0msep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmsg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 583\u001B[0;31m     \u001B[0;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresource_not_found\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    584\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords.zip/stopwords/\u001B[0m\n\n  Searched in:\n    - '/Users/jeevavighnesh/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-d55c035ab564>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mstop_words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstopwords\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'english'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/corpus/util.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, attr)\u001B[0m\n\u001B[1;32m    118\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__load\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m         \u001B[0;31m# This looks circular, but its not, since __load() changes our\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m         \u001B[0;31m# __class__ to something new:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/corpus/util.py\u001B[0m in \u001B[0;36m__load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     83\u001B[0m                     \u001B[0mroot\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"{}/{}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msubdir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mzip_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     84\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 85\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     86\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m         \u001B[0;31m# Load the corpus.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/corpus/util.py\u001B[0m in \u001B[0;36m__load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     78\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     79\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 80\u001B[0;31m                 \u001B[0mroot\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"{}/{}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msubdir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     81\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/data.py\u001B[0m in \u001B[0;36mfind\u001B[0;34m(resource_name, paths)\u001B[0m\n\u001B[1;32m    581\u001B[0m     \u001B[0msep\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"*\"\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m70\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    582\u001B[0m     \u001B[0mresource_not_found\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0msep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmsg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 583\u001B[0;31m     \u001B[0;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresource_not_found\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    584\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - '/Users/jeevavighnesh/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9WUR452uWtNP"
   },
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "data = pd.read_csv(\n",
    "    r\"http://storage.googleapis.com/kagglesdsdata/datasets/22169/30047/sentiment%20labelled%20sentences/imdb_labelled.txt?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210425%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210425T202010Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=6133706ef10bc2dcd0b58f8398b4d73ab9e9d788de1718b07334df91f6007e1e4ca0b78e3176f95b8250e0c4535ce1633528f4fabffeb7e4124af3ee3f895ac34c03044fca9b23b23c4ddb8fa90d84dfc14869ff4806f03783cafad53b19445b3c3052983fdf1ca4384257eac1bc0a4270d238a1ea89d1289866c7a0ea7ad7c97a76f2e142c148019e39cc5a1295f92650747ac5ea5946b026f7ad6d5d262d4c4a370aee6bc1f5d5b445bb6d93692debe678a79e5e1c1fe3d3e68ea4f2fad3115795d3361e0626e98156fbc7f5967beb7cf0f00e07351d23a00d8677ebb75e3e13b1bfa07762266efabf6f6f9d53206be31b7623cf3614f60f8cf5011cf23def\",\n",
    "    delimiter=\"\\t\", header=None, names=[REVIEW_COL, SENTIMENT_COL])\n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "exqp_Lz4WtNQ"
   },
   "source": [
    "### Split Data\n",
    "We split the data into train, development and test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xic29f9uWtNQ"
   },
   "source": [
    "train = data[:math.floor(data.shape[0] * TRAINING_RATIO)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yhXs8XenWtNQ"
   },
   "source": [
    "validation = data[math.floor(data.shape[0] * TRAINING_RATIO):].sample(frac=1).reset_index(drop=True)\n",
    "dev, test = np.array_split(validation, 2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "tm__yqxIWtNQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "3f9d4669-f0c2-44a8-a773-13ee3261f82d"
   },
   "source": [
    "display(train, dev, test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "narVDsEyWtNR"
   },
   "source": [
    "## Generation of Vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ndy_o4PvWtNR"
   },
   "source": [
    "def split_words(review: str, remove_stop_words: bool):\n",
    "    reviews_array = review.lower().replace(',', '').replace('\"', '').replace('(', '').replace(')', '').replace('\\'s',\n",
    "                                                                                                               '').replace(\n",
    "        '.',\n",
    "        '').replace(\n",
    "        '!', '').replace('/', ' ').split()\n",
    "    return [w for w in reviews_array if not w in stop_words] if remove_stop_words else reviews_array\n",
    "\n",
    "\n",
    "def get_word_count(review_data_frame: pd.DataFrame, column_name: str, remove_stop_words: bool):\n",
    "    vocab = review_data_frame[REVIEW_COL].apply(lambda review: pd.value_counts(\n",
    "        split_words(review, remove_stop_words))).count(axis=0).to_frame()\n",
    "    vocab.columns = [column_name]\n",
    "    vocab.reset_index(inplace=True)\n",
    "    vocab = vocab.rename(columns={'index': WORD_COL})\n",
    "    return vocab"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8_SVhXUXZI5"
   },
   "source": [
    "\n",
    "## Get Naive Bayes Parameters\n",
    "Here we have a function to genereate the Naive Bayes Parameters like:\n",
    "\n",
    "1. Word Frequency\n",
    "2. P(Word)\n",
    "3. Positive Sentiment Word Frequency\n",
    "4. P(Sentiment = Positive)\n",
    "5. P(Word | Sentiment = Positive)\n",
    "6. Negative Sentiment Word Frequency\n",
    "7. P(Sentiment = Negative)\n",
    "8. P(Word | Sentiment = Negative)\n",
    "\n",
    "Which are useful in finding:\n",
    "\n",
    "**P(Sentiment | Sentence (Collection of words)) = P(Sentence | Sentiment) * P(Sentiment) / P(Sentense)**\n",
    "\n",
    "The P(Sentense) can be approximated to 1 as we are comparing sentiments the value will be cancelled on either sides\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "E9mp3SLjWtNR"
   },
   "source": [
    "def generate_naive_bayes_parameters(data_frame: pd.DataFrame, smoothening: bool, remove_stop_words: bool):\n",
    "    naive_bayes_parameters = get_word_count(data_frame, WORD_FREQ_COL, remove_stop_words)\n",
    "    if smoothening:\n",
    "        naive_bayes_parameters[WORD_FREQ_COL] += 1\n",
    "\n",
    "    total_words = naive_bayes_parameters[WORD_FREQ_COL].sum(axis=0)\n",
    "    if smoothening:\n",
    "        total_words += 2\n",
    "\n",
    "    total_sentiments = data_frame.count(axis=0)[SENTIMENT_COL]\n",
    "    if smoothening:\n",
    "        total_sentiments += 2\n",
    "\n",
    "    naive_bayes_parameters[P_WORD_COL] = naive_bayes_parameters[WORD_FREQ_COL].div(total_words)\n",
    "\n",
    "    positive_sentiments = data_frame[data_frame[SENTIMENT_COL] == 1]\n",
    "    positive_vocabulary = get_word_count(positive_sentiments, POS_SENTIMENT_WORD_FREQ_COL, remove_stop_words)\n",
    "    naive_bayes_parameters = naive_bayes_parameters.merge(positive_vocabulary, how='left', on=WORD_COL)\n",
    "    if smoothening:\n",
    "        naive_bayes_parameters[POS_SENTIMENT_WORD_FREQ_COL] += 1\n",
    "        naive_bayes_parameters[POS_SENTIMENT_WORD_FREQ_COL] = naive_bayes_parameters[\n",
    "            POS_SENTIMENT_WORD_FREQ_COL].fillna(\n",
    "            value=1)\n",
    "\n",
    "    total_positive_words = positive_sentiments.count(axis=0)[SENTIMENT_COL]\n",
    "    if smoothening:\n",
    "        total_positive_words += 2\n",
    "\n",
    "    probability_of_positive_sentiments = total_positive_words / total_sentiments\n",
    "    naive_bayes_parameters[P_SENTIMENT_POSITIVE_COL] = probability_of_positive_sentiments\n",
    "\n",
    "    naive_bayes_parameters[P_WORD_GIVEN_SENTIMENT_POSITIVE_COL] = naive_bayes_parameters[\n",
    "        POS_SENTIMENT_WORD_FREQ_COL].div(\n",
    "        total_positive_words)\n",
    "\n",
    "    negative_sentiments = data_frame[data_frame[SENTIMENT_COL] == 0]\n",
    "    negative_vocabulary = get_word_count(negative_sentiments, NEG_SENTIMENT_WORD_FREQ_COL, remove_stop_words)\n",
    "    naive_bayes_parameters = naive_bayes_parameters.merge(negative_vocabulary, how='left', on=WORD_COL)\n",
    "    if smoothening:\n",
    "        naive_bayes_parameters[NEG_SENTIMENT_WORD_FREQ_COL] += 1\n",
    "        naive_bayes_parameters[NEG_SENTIMENT_WORD_FREQ_COL] = naive_bayes_parameters[\n",
    "            NEG_SENTIMENT_WORD_FREQ_COL].fillna(\n",
    "            value=1)\n",
    "\n",
    "    total_negative_words = negative_sentiments.count(axis=0)[SENTIMENT_COL]\n",
    "    if smoothening:\n",
    "        total_negative_words += 2\n",
    "\n",
    "    probability_of_negative_sentiments = total_negative_words / total_sentiments\n",
    "    naive_bayes_parameters[P_SENTIMENT_NEGATIVE_COL] = probability_of_negative_sentiments\n",
    "\n",
    "    naive_bayes_parameters[P_WORD_GIVEN_SENTIMENT_NEGATIVE_COL] = naive_bayes_parameters[\n",
    "        NEG_SENTIMENT_WORD_FREQ_COL].div(\n",
    "        total_negative_words)\n",
    "\n",
    "    return naive_bayes_parameters"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clFkY7FPfn0d"
   },
   "source": [
    "\n",
    "## To Get the Probabilities\n",
    "\n",
    "We use this formula to get the probabilities:\n",
    "\n",
    "**P(Sentiment | Sentence (Collection of words)) = P(Sentence | Sentiment) * P(Sentiment) / P(Sentense)**\n",
    "\n",
    "The below function will calculate the numerator part and assumes the denominator to be 1 as it will cancel out during\n",
    "comparison.\n",
    "\n",
    "For calculating the P(Sentence | Sentiment) we have words in sentences. So, we can write the formula as:\n",
    "\n",
    "**P(Sentence | Sentiment) = P(Word_1,Word_2,...,Word_n | Sentiment)**\n",
    "\n",
    "By Naive Bayes Theorem we can write it as:\n",
    "\n",
    "**P(Word_1,Word_2,...,Word_n | Sentiment) = P(Word_1 | Sentiment).P(Word_2 | Sentiment). ... .P(Word_n | Sentiment)**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "U2Bx8TIeWtNS"
   },
   "source": [
    "def get_probabilities(review: str, naive_bayes_parameters: pd.DataFrame, sentiment: bool, smoothening: bool,\n",
    "                      remove_stop_words: bool):\n",
    "    prob = 1\n",
    "    column_name = P_WORD_GIVEN_SENTIMENT_POSITIVE_COL if sentiment else P_WORD_GIVEN_SENTIMENT_NEGATIVE_COL\n",
    "    individual_prob = 0 if not smoothening else 1 / (\n",
    "        naive_bayes_parameters[P_SENTIMENT_POSITIVE_COL][0] if sentiment else naive_bayes_parameters[\n",
    "            P_SENTIMENT_NEGATIVE_COL][0])\n",
    "    for word in split_words(review, remove_stop_words):\n",
    "        if word in naive_bayes_parameters.values:\n",
    "            individual_prob = naive_bayes_parameters[naive_bayes_parameters[WORD_COL] == word].iloc[0][column_name]\n",
    "        prob *= 0 if math.isnan(individual_prob) else individual_prob\n",
    "    return prob * (naive_bayes_parameters[P_SENTIMENT_POSITIVE_COL][0] if sentiment else naive_bayes_parameters[\n",
    "        P_SENTIMENT_NEGATIVE_COL][0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Wh8laKf9WtNS"
   },
   "source": [
    "def predict_calculate_accuracy(data_frame: pd.DataFrame, naive_bayes_parameters: pd.DataFrame, smoothening: bool,\n",
    "                               remove_stop_words: bool):\n",
    "    data_frame[P_SENTIMENT_POSITIVE_GIVEN_SENTENCE_COL] = data_frame[REVIEW_COL].apply(\n",
    "        lambda review: get_probabilities(review, naive_bayes_parameters, True, smoothening, remove_stop_words))\n",
    "    data_frame[P_SENTIMENT_NEGATIVE_GIVEN_SENTENCE_COL] = data_frame[REVIEW_COL].apply(\n",
    "        lambda review: get_probabilities(review, naive_bayes_parameters, False, smoothening, remove_stop_words))\n",
    "    data_frame[PREDICTED_SENTIMENT_COL] = data_frame[P_SENTIMENT_POSITIVE_GIVEN_SENTENCE_COL] > data_frame[\n",
    "        P_SENTIMENT_NEGATIVE_GIVEN_SENTENCE_COL]\n",
    "    accuracy = data_frame.loc[data_frame[PREDICTED_SENTIMENT_COL] == data_frame[SENTIMENT_COL]].count(axis=0)[\n",
    "                   SENTIMENT_COL] * 100 / data_frame.count(axis=0)[SENTIMENT_COL]\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    # print(\"Wrong Predictions:\")\n",
    "    # display(data_frame.loc[data_frame[PREDICTED_SENTIMENT_COL] != data_frame[SENTIMENT_COL]].reset_index(drop=True))\n",
    "    return accuracy\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZXRLYDi1oyw"
   },
   "source": [
    "## Calculating Accuracy\n",
    "\n",
    "To calculate accuracy we first divide the training dataset into k parts of train and test the first part of the\n",
    "set is used to train the dataset with the remaining k-1 test dataset.\n",
    "\n",
    "We then predict using the Naive bayes parameters that we get from training against the test data.\n",
    "\n",
    "We then calculate the accuracy by finding (how many data is of correct prediction)/(total number of datasets)\n",
    "\n",
    "With the parameters having the best accuracy is chosen from this and used for further validation of dev anf test\n",
    "datasets which we separated in the beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "irEIRcRXWtNS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "73bfca5d-2bdf-4c80-bb13-c1f77829cf7b"
   },
   "source": [
    "def five_fold_cross_validation(data_frame: pd.DataFrame, smoothening: bool, remove_stop_words: bool):\n",
    "    kf = KFold(n_splits=K_FOLDS, shuffle=True)\n",
    "    train_folds = kf.split(data_frame)\n",
    "    accuracies = []\n",
    "    max_accuracy_naive_bayes_parameters = pd.DataFrame()\n",
    "    for (train_training, train_testing), index in zip(train_folds, range(5)):\n",
    "        print(f\"---------------------------Fold {index + 1}---------------------------------\")\n",
    "        display(train.loc[train_training])\n",
    "        trained_parameters = generate_naive_bayes_parameters(train.loc[train_training], smoothening, remove_stop_words)\n",
    "        accuracy = predict_calculate_accuracy(train.loc[train_testing], trained_parameters, smoothening,\n",
    "                                              remove_stop_words)\n",
    "        accuracies.append(accuracy)\n",
    "        max_accuracy_naive_bayes_parameters = trained_parameters if max(\n",
    "            accuracies) == accuracy else max_accuracy_naive_bayes_parameters\n",
    "        display(trained_parameters)\n",
    "    return max_accuracy_naive_bayes_parameters\n",
    "\n",
    "\n",
    "vocabulary = five_fold_cross_validation(train, False, False)\n",
    "vocabulary"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "flA2Y4QvWtNS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0a6cc491-ad46-4779-dbf4-f167988d5401"
   },
   "source": [
    "predict_calculate_accuracy(train, vocabulary, False, False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "q9T3yQIlWtNT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "35170586-9b8d-4213-a05f-b8a5388e855b"
   },
   "source": [
    "predict_calculate_accuracy(dev, vocabulary, False, False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "X1pkThsMWtNT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b383a28b-baae-4abf-d8dc-b1de8fa09817"
   },
   "source": [
    "predict_calculate_accuracy(test, vocabulary, False, False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "IwvhhaSbWtNT"
   },
   "source": [
    "## Most Useful words before Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ZPTG_pKtWtNT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "outputId": "65175c9c-0187-4c26-a25e-aa21556c164a"
   },
   "source": [
    "print(\"Most Useful Positive sentiment words:\")\n",
    "vocabulary.sort_values(P_WORD_GIVEN_SENTIMENT_POSITIVE_COL, ascending=False)[:MOST_USEFUL_LIMIT]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "48fYT2uAWtNT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "outputId": "eb151fcc-c6df-4754-d8d4-c02b6321e7a5"
   },
   "source": [
    "print(\"Most Useful Negative sentiment words:\")\n",
    "vocabulary.sort_values(P_WORD_GIVEN_SENTIMENT_NEGATIVE_COL, ascending=False)[:MOST_USEFUL_LIMIT]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "srV9dlcLWtNT"
   },
   "source": [
    "## Smoothening\n",
    "\n",
    "Smoothening is done to compensate for unknown words. As all words can't be added to a dictionary and Naive Bayes is\n",
    "specialized to handle missing words.\n",
    "\n",
    "Smoothening is done by using the +1 method it is done in the get_naive_bayes_parameters function.\n",
    "\n",
    "All it does is adding +1 to the following:\n",
    "1. Word Frequency\n",
    "2. Positive Sentiment Word Frequency\n",
    "3. Negative Sentiment Word Frequency\n",
    "\n",
    "Also, +2 for Number of sentiments as these terms are in the denominator and needs to adhere and compensate for the +1 in\n",
    "the numerator so that the probability of most occurrence words will be less than 1\n",
    "1. Total words\n",
    "2. Total Positive sentiments\n",
    "3. Total Negative sentiments\n",
    "4. Total sentiments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SAP708JGWtNT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "282e5548-fab7-4357-b1c0-e06c5347448c"
   },
   "source": [
    "vocabulary = five_fold_cross_validation(train, True, False)\n",
    "vocabulary"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "7GlKzJJWWtNT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a0dcad09-141b-453a-c76f-d1d64609a77e"
   },
   "source": [
    "predict_calculate_accuracy(train, vocabulary, True, False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ZR3NgMpQWtNT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b501e2bd-bf14-4a6e-979a-d29789e539dc"
   },
   "source": [
    "predict_calculate_accuracy(dev, vocabulary, True, False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6RdTL_cvWtNU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "613e6a0b-ac1b-4e78-8f5e-6078d0d967d1"
   },
   "source": [
    "predict_calculate_accuracy(test, vocabulary, True, False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OcZ6QdNHWtNU"
   },
   "source": [
    "## Most Useful words after Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "GtNHkJYTWtNU",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "outputId": "68611069-0459-4606-dc1e-dc93f1b57dd5"
   },
   "source": [
    "print(\"Most Useful Positive sentiment words:\")\n",
    "vocabulary.sort_values(P_WORD_GIVEN_SENTIMENT_POSITIVE_COL, ascending=False)[:MOST_USEFUL_LIMIT]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "qtbvAXuTWtNU",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "outputId": "1bb23676-1619-4f9a-c23d-09c256cfcfe0"
   },
   "source": [
    "print(\"Most Useful Negative sentiment words:\")\n",
    "vocabulary.sort_values(P_WORD_GIVEN_SENTIMENT_NEGATIVE_COL, ascending=False)[:MOST_USEFUL_LIMIT]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2A6Iw3Aet72T",
    "outputId": "faa1f35f-fbec-4c22-d03c-53128acd88c4"
   },
   "source": [
    "vocabulary = five_fold_cross_validation(train, True, True)\n",
    "vocabulary"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3zPiu0AyaTf"
   },
   "source": [
    "## Inference\n",
    "\n",
    "From the above we can see the effect of smoothening at the time of runtime with accuracy increase of 15%.\n",
    "\n",
    "Also, from the most useful words we can see 2 things.\n",
    "1. The most common words are the useful words.\n",
    "2. The most common words, and some words have higher probability in both positive and negative sentiments.\n",
    "\n",
    "This shows us that these data need to be removed.\n",
    "\n",
    "For doing these as future work we can remove stop words from Pythons old NLTK library for stop words.\n",
    "Also, we can remove the more frequent words like the movie, film as it is both positive and negative which is\n",
    "logical as it is a movie database...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nm5epGYVyl69"
   },
   "source": [
    "## Removing Stop Words and positive and negative words [Bonus Experiment]\n",
    "\n",
    "For this as described in the inference we use the NLTK Library and add the most positive and negative words to the stop words."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FJGmEHqozeKl"
   },
   "source": [
    "for common_words in MOST_COMMON_WORDS_IN_DATA_SET: stop_words.append(common_words)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_WIkDpaO1YuX",
    "outputId": "46d03545-9daa-4f9b-8b18-cde187ac2e35"
   },
   "source": [
    "vocabulary = five_fold_cross_validation(train, True, True)\n",
    "vocabulary"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1DwjFfdt9ym",
    "outputId": "a5a4fc39-e619-4144-e529-be44a3b61258"
   },
   "source": [
    "predict_calculate_accuracy(train, vocabulary, True, True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7Pd-nONt9YQ",
    "outputId": "fdd7ac2a-41e4-402c-a230-8ac5063111a5"
   },
   "source": [
    "predict_calculate_accuracy(dev, vocabulary, True, True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8ypMZGSt9Qk",
    "outputId": "f326aeec-0872-4acd-eb66-895e7d22a6f7"
   },
   "source": [
    "predict_calculate_accuracy(test, vocabulary, True, True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "eQOESagqvXEF",
    "outputId": "e4636fc9-656b-4e39-8178-0f87736ee7a9"
   },
   "source": [
    "print(\"Most Useful Positive sentiment words:\")\n",
    "vocabulary.sort_values(P_WORD_GIVEN_SENTIMENT_POSITIVE_COL, ascending=False)[:MOST_USEFUL_LIMIT]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "AfILx1Q6vXvf",
    "outputId": "4c816477-4076-4e3b-f9be-4921df7086dd"
   },
   "source": [
    "print(\"Most Useful Negative sentiment words:\")\n",
    "vocabulary.sort_values(P_WORD_GIVEN_SENTIMENT_NEGATIVE_COL, ascending=False)[:MOST_USEFUL_LIMIT]"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}